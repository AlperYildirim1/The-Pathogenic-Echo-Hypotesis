{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNR6IWKs9qX4MNDe+h56LHy",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AlperYildirim1/The-Pathogenic-Echo-Hypotesis/blob/main/Kauvar.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GYsPqqtRbSho",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "!pip install awscli"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!aws s3 sync s3://openneuro.org/ds003004 ./ds003004 --no-sign-request"
      ],
      "metadata": {
        "id": "Mc8cbrCfcUKM",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================\n",
        "# EXPLORATION SCRIPT for ds003004\n",
        "# Goal: Understand the data structure and event timings.\n",
        "# =============================================================\n",
        "\n",
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "\n",
        "dataset_path = './ds003004'\n",
        "subject = 'sub-01'\n",
        "events_file_path = os.path.join(dataset_path, subject, 'eeg', f'{subject}_task-ImaginedEmotion_events.tsv')\n",
        "\n",
        "print(f\"Attempting to load events file from: {events_file_path}\")\n",
        "\n",
        "try:\n",
        "\n",
        "    events_df = pd.read_csv(events_file_path, sep='\\t')\n",
        "\n",
        "    print(\"\\n✅ Successfully loaded the events file. Here are the first 15 rows:\")\n",
        "    print(\"--------------------------------------------------------------------\")\n",
        "\n",
        "    print(events_df.head(15).to_string())\n",
        "    print(\"--------------------------------------------------------------------\")\n",
        "\n",
        "    print(\"\\nLet's look at the unique event types (trial_type column):\")\n",
        "    print(events_df['trial_type'].unique())\n",
        "    print(\"\\n--------------------------------------------------------------------\")\n",
        "\n",
        "    print(\"\\nNow, let's look at the full table for a single emotion, for example 'sadness':\")\n",
        "    sadness_events = events_df[events_df['trial_type'] == 'sadness']\n",
        "    print(sadness_events.to_string())\n",
        "\n",
        "\n",
        "except FileNotFoundError:\n",
        "    print(f\"\\n❌ FILE NOT FOUND. Could not find the events.tsv file at the specified path.\")\n",
        "    print(\"Please make sure you have run the download command and the path is correct.\")\n",
        "except Exception as e:\n",
        "    print(f\"\\n❌ AN ERROR OCCURRED: {e}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bFpcNzvFeEWt",
        "outputId": "3bbb68b6-eed9-4c37-ad71-e2b13c7aa9ef"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Attempting to load events file from: ./ds003004/sub-01/eeg/sub-01_task-ImaginedEmotion_events.tsv\n",
            "\n",
            "✅ Successfully loaded the events file. Here are the first 15 rows:\n",
            "--------------------------------------------------------------------\n",
            "         onset  duration  sample  trial_type  response_time  stim_file                        value  HED\n",
            "0     6.343750       0.0     NaN         NaN            NaN        NaN          InitialInstructions  NaN\n",
            "1     6.734375       0.0     NaN         NaN            NaN        NaN             prebase_instruct  NaN\n",
            "2   117.503906       0.0     NaN         NaN            NaN        NaN                      prebase  NaN\n",
            "3   237.503906       0.0     NaN         NaN            NaN        NaN                         exit  NaN\n",
            "4   237.507812       0.0     NaN         NaN            NaN        NaN  FeelingItInstructionsButton  NaN\n",
            "5   237.781250       0.0     NaN         NaN            NaN        NaN        InstructionsForEnding  NaN\n",
            "6   304.789062       0.0     NaN         NaN            NaN        NaN                        relax  NaN\n",
            "7   368.820312       0.0     NaN         NaN            NaN        NaN       ImaginationSuggestions  NaN\n",
            "8   716.136719       0.0     NaN         NaN            NaN        NaN                        enter  NaN\n",
            "9   716.269531       0.0     NaN         NaN            NaN        NaN                          awe  NaN\n",
            "10  780.382812       0.0     NaN         NaN            NaN        NaN                       press1  NaN\n",
            "11  782.332031       0.0     NaN         NaN            NaN        NaN                        press  NaN\n",
            "12  783.765625       0.0     NaN         NaN            NaN        NaN                        press  NaN\n",
            "13  798.164062       0.0     NaN         NaN            NaN        NaN                        press  NaN\n",
            "14  818.445312       0.0     NaN         NaN            NaN        NaN                        press  NaN\n",
            "--------------------------------------------------------------------\n",
            "\n",
            "Let's look at the unique event types (trial_type column):\n",
            "[nan]\n",
            "\n",
            "--------------------------------------------------------------------\n",
            "\n",
            "Now, let's look at the full table for a single emotion, for example 'sadness':\n",
            "Empty DataFrame\n",
            "Columns: [onset, duration, sample, trial_type, response_time, stim_file, value, HED]\n",
            "Index: []\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================\n",
        "# TEMPORAL GENERALIZATION SCRIPT for ds003004\n",
        "# Goal: Train a model on Phase 1 and test it on Phase 2.\n",
        "# =============================================================\n",
        "\n",
        "# --- STEP 0: INSTALL MNE-PYTHON ---\n",
        "!pip install mne -q\n",
        "\n",
        "import mne\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "from sklearn.model_selection import StratifiedKFold, cross_val_score\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# --- STEP 1: DEFINE FILE PATHS ---\n",
        "dataset_path = './ds003004'\n",
        "subject = 'sub-01'\n",
        "eeg_file_path = os.path.join(dataset_path, subject, 'eeg', f'{subject}_task-ImaginedEmotion_eeg.set')\n",
        "events_file_path = os.path.join(dataset_path, subject, 'eeg', f'{subject}_task-ImaginedEmotion_events.tsv')\n",
        "\n",
        "print(f\"Loading EEG data from: {eeg_file_path}\")\n",
        "print(f\"Loading events from: {events_file_path}\")\n",
        "\n",
        "try:\n",
        "    # --- STEP 2: LOAD EEG DATA AND EVENTS ---\n",
        "    raw = mne.io.read_raw_eeglab(eeg_file_path, preload=True)\n",
        "    events_df = pd.read_csv(events_file_path, sep='\\t')\n",
        "\n",
        "    raw.filter(l_freq=1.0, h_freq=None, n_jobs=-1)\n",
        "\n",
        "    print(\"\\n✅ Data and events loaded successfully.\")\n",
        "\n",
        "    # --- STEP 3: PARSE EVENTS TO DEFINE TRIALS AND PHASES ---\n",
        "    print(\"--- Parsing events to define emotion trials and phases ---\")\n",
        "\n",
        "    emotion_list = [\n",
        "        'awe', 'frustration', 'joy', 'anger', 'happy', 'sad',\n",
        "        'love', 'fear', 'compassion', 'jealousy', 'content',\n",
        "        'grief', 'relief', 'disgust', 'excitement'\n",
        "    ]\n",
        "\n",
        "    phase1_epochs_list = []\n",
        "    phase2_epochs_list = []\n",
        "    labels_list = []\n",
        "\n",
        "    for i, emotion in enumerate(emotion_list):\n",
        "        emotion_event = events_df[events_df['value'] == emotion]\n",
        "        if emotion_event.empty: continue\n",
        "\n",
        "        t_start = emotion_event['onset'].iloc[0]\n",
        "\n",
        "        press1_event = events_df[(events_df['value'] == 'press1') & (events_df['onset'] > t_start)]\n",
        "        if press1_event.empty: continue\n",
        "        t_button1 = press1_event['onset'].iloc[0]\n",
        "\n",
        "        end_event = events_df[(events_df['value'].isin(['relax', 'exit'])) & (events_df['onset'] > t_button1)]\n",
        "        if end_event.empty: continue\n",
        "        t_end = end_event['onset'].iloc[0]\n",
        "\n",
        "        phase1_duration = t_button1 - t_start\n",
        "        if phase1_duration <= 0: continue\n",
        "        events_phase1 = np.array([[int(t_start * raw.info['sfreq']), 0, i+1]])\n",
        "        epochs_p1 = mne.Epochs(raw, events_phase1, event_id=i+1, tmin=0, tmax=phase1_duration, preload=True, baseline=None)\n",
        "\n",
        "        phase2_duration = t_end - t_button1\n",
        "        if phase2_duration <= 0: continue\n",
        "        events_phase2 = np.array([[int(t_button1 * raw.info['sfreq']), 0, i+1]])\n",
        "        epochs_p2 = mne.Epochs(raw, events_phase2, event_id=i+1, tmin=0, tmax=phase2_duration, preload=True, baseline=None)\n",
        "\n",
        "        if len(epochs_p1) > 0 and len(epochs_p2) > 0:\n",
        "            phase1_epochs_list.append(epochs_p1.get_data())\n",
        "            phase2_epochs_list.append(epochs_p2.get_data())\n",
        "\n",
        "            if emotion in ['frustration', 'anger', 'sad', 'fear', 'jealousy', 'grief', 'disgust']:\n",
        "                labels_list.append('Negative')\n",
        "            else:\n",
        "                labels_list.append('Positive')\n",
        "\n",
        "    y_labels = np.array(labels_list)\n",
        "\n",
        "    print(f\"\\n✅ Successfully created {len(y_labels)} epochs for Phase 1 and Phase 2.\")\n",
        "\n",
        "    # --- STEP 4: CALCULATE FEATURES (PSD) ---\n",
        "    print(\"--- Calculating Power Spectral Density (PSD) features ---\")\n",
        "\n",
        "    def compute_psd_for_list(epoch_list, sfreq):\n",
        "        psd_features = []\n",
        "        for epoch_data in epoch_list:\n",
        "\n",
        "            psd, freqs = mne.time_frequency.psd_array_welch(epoch_data, sfreq=sfreq, fmin=1, fmax=45, n_fft=256, average='mean')\n",
        "\n",
        "            psd_features.append(psd.flatten())\n",
        "        return np.array(psd_features)\n",
        "\n",
        "    X_phase1 = compute_psd_for_list(phase1_epochs_list, raw.info['sfreq'])\n",
        "    X_phase2 = compute_psd_for_list(phase2_epochs_list, raw.info['sfreq'])\n",
        "\n",
        "    print(f\"Feature matrix shape for Phase 1: {X_phase1.shape}\")\n",
        "    print(f\"Feature matrix shape for Phase 2: {X_phase2.shape}\")\n",
        "\n",
        "    # --- STEP 5: TRAIN ON PHASE 1, TEST ON PHASE 2 ---\n",
        "    print(\"\\n--- STEP 5: Training on Phase 1, Testing on Phase 2 ---\")\n",
        "\n",
        "    if len(y_labels) < 5:\n",
        "        raise ValueError(\"Not enough valid trials found to perform classification.\")\n",
        "\n",
        "    X_train = X_phase1\n",
        "    X_test = X_phase2\n",
        "\n",
        "    clf = make_pipeline(StandardScaler(), SVC(kernel='linear'))\n",
        "    clf.fit(X_train, y_labels)\n",
        "\n",
        "    accuracy = clf.score(X_test, y_labels)\n",
        "\n",
        "    print(\"\\n================================================\")\n",
        "    print(\"       TEMPORAL GENERALIZATION RESULTS\")\n",
        "    print(\"================================================\")\n",
        "    print(f\"Model trained on 'Formation' Phase (Phase 1), tested on 'Persistent' Phase (Phase 2).\")\n",
        "    print(f\"Accuracy: {accuracy:.2%}\")\n",
        "    print(\"This result shows how well the 'formation' signature predicts the 'persistent' state.\")\n",
        "    print(\"Chance level for Positive vs. Negative is ~50%.\")\n",
        "\n",
        "    cv = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)\n",
        "    scores_phase2 = cross_val_score(clf, X_test, y_labels, cv=cv, scoring='accuracy')\n",
        "    print(f\"\\nFor reference, internal classification accuracy of Phase 2 data: {np.mean(scores_phase2):.2%}\")\n",
        "\n",
        "except FileNotFoundError:\n",
        "    print(f\"\\n❌ FILE NOT FOUND. Could not find EEG data at the specified path.\")\n",
        "except Exception as e:\n",
        "    print(f\"\\n❌ AN ERROR OCCURRED: {e}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fboB-7LSe9ft",
        "outputId": "916b7978-06f2-407b-d6f5-c300beff5a18"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/7.4 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.3/7.4 MB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.9/7.4 MB\u001b[0m \u001b[31m43.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m7.4/7.4 MB\u001b[0m \u001b[31m84.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.4/7.4 MB\u001b[0m \u001b[31m67.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hLoading EEG data from: ./ds003004/sub-01/eeg/sub-01_task-ImaginedEmotion_eeg.set\n",
            "Loading events from: ./ds003004/sub-01/eeg/sub-01_task-ImaginedEmotion_events.tsv\n",
            "Reading /content/ds003004/sub-01/eeg/sub-01_task-ImaginedEmotion_eeg.fdt\n",
            "Reading 0 ... 1154303  =      0.000 ...  4508.996 secs...\n",
            "Filtering raw data in 1 contiguous segment\n",
            "Setting up high-pass filter at 1 Hz\n",
            "\n",
            "FIR filter parameters\n",
            "---------------------\n",
            "Designing a one-pass, zero-phase, non-causal highpass filter:\n",
            "- Windowed time-domain design (firwin) method\n",
            "- Hamming window with 0.0194 passband ripple and 53 dB stopband attenuation\n",
            "- Lower passband edge: 1.00\n",
            "- Lower transition bandwidth: 1.00 Hz (-6 dB cutoff frequency: 0.50 Hz)\n",
            "- Filter length: 845 samples (3.301 s)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done  16 tasks      | elapsed:    2.5s\n",
            "[Parallel(n_jobs=-1)]: Done 202 tasks      | elapsed:   13.0s\n",
            "[Parallel(n_jobs=-1)]: Done 224 out of 224 | elapsed:   14.7s finished\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "✅ Data and events loaded successfully.\n",
            "--- Parsing events to define emotion trials and phases ---\n",
            "Not setting metadata\n",
            "1 matching events found\n",
            "No baseline correction applied\n",
            "0 projection items activated\n",
            "Using data from preloaded Raw for 1 events and 16414 original time points ...\n",
            "0 bad epochs dropped\n",
            "Not setting metadata\n",
            "1 matching events found\n",
            "No baseline correction applied\n",
            "0 projection items activated\n",
            "Using data from preloaded Raw for 1 events and 32669 original time points ...\n",
            "0 bad epochs dropped\n",
            "Not setting metadata\n",
            "1 matching events found\n",
            "No baseline correction applied\n",
            "0 projection items activated\n",
            "Using data from preloaded Raw for 1 events and 20140 original time points ...\n",
            "0 bad epochs dropped\n",
            "Not setting metadata\n",
            "1 matching events found\n",
            "No baseline correction applied\n",
            "0 projection items activated\n",
            "Using data from preloaded Raw for 1 events and 48603 original time points ...\n",
            "0 bad epochs dropped\n",
            "Not setting metadata\n",
            "1 matching events found\n",
            "No baseline correction applied\n",
            "0 projection items activated\n",
            "Using data from preloaded Raw for 1 events and 21863 original time points ...\n",
            "0 bad epochs dropped\n",
            "Not setting metadata\n",
            "1 matching events found\n",
            "No baseline correction applied\n",
            "0 projection items activated\n",
            "Using data from preloaded Raw for 1 events and 26788 original time points ...\n",
            "0 bad epochs dropped\n",
            "Not setting metadata\n",
            "1 matching events found\n",
            "No baseline correction applied\n",
            "0 projection items activated\n",
            "Using data from preloaded Raw for 1 events and 14682 original time points ...\n",
            "0 bad epochs dropped\n",
            "Not setting metadata\n",
            "1 matching events found\n",
            "No baseline correction applied\n",
            "0 projection items activated\n",
            "Using data from preloaded Raw for 1 events and 34028 original time points ...\n",
            "0 bad epochs dropped\n",
            "Not setting metadata\n",
            "1 matching events found\n",
            "No baseline correction applied\n",
            "0 projection items activated\n",
            "Using data from preloaded Raw for 1 events and 17338 original time points ...\n",
            "0 bad epochs dropped\n",
            "Not setting metadata\n",
            "1 matching events found\n",
            "No baseline correction applied\n",
            "0 projection items activated\n",
            "Using data from preloaded Raw for 1 events and 37009 original time points ...\n",
            "0 bad epochs dropped\n",
            "Not setting metadata\n",
            "1 matching events found\n",
            "No baseline correction applied\n",
            "0 projection items activated\n",
            "Using data from preloaded Raw for 1 events and 22200 original time points ...\n",
            "0 bad epochs dropped\n",
            "Not setting metadata\n",
            "1 matching events found\n",
            "No baseline correction applied\n",
            "0 projection items activated\n",
            "Using data from preloaded Raw for 1 events and 28561 original time points ...\n",
            "0 bad epochs dropped\n",
            "Not setting metadata\n",
            "1 matching events found\n",
            "No baseline correction applied\n",
            "0 projection items activated\n",
            "Using data from preloaded Raw for 1 events and 15286 original time points ...\n",
            "0 bad epochs dropped\n",
            "Not setting metadata\n",
            "1 matching events found\n",
            "No baseline correction applied\n",
            "0 projection items activated\n",
            "Using data from preloaded Raw for 1 events and 26973 original time points ...\n",
            "0 bad epochs dropped\n",
            "Not setting metadata\n",
            "1 matching events found\n",
            "No baseline correction applied\n",
            "0 projection items activated\n",
            "Using data from preloaded Raw for 1 events and 16517 original time points ...\n",
            "0 bad epochs dropped\n",
            "Not setting metadata\n",
            "1 matching events found\n",
            "No baseline correction applied\n",
            "0 projection items activated\n",
            "Using data from preloaded Raw for 1 events and 14145 original time points ...\n",
            "0 bad epochs dropped\n",
            "Not setting metadata\n",
            "1 matching events found\n",
            "No baseline correction applied\n",
            "0 projection items activated\n",
            "Using data from preloaded Raw for 1 events and 13216 original time points ...\n",
            "0 bad epochs dropped\n",
            "Not setting metadata\n",
            "1 matching events found\n",
            "No baseline correction applied\n",
            "0 projection items activated\n",
            "Using data from preloaded Raw for 1 events and 36962 original time points ...\n",
            "0 bad epochs dropped\n",
            "Not setting metadata\n",
            "1 matching events found\n",
            "No baseline correction applied\n",
            "0 projection items activated\n",
            "Using data from preloaded Raw for 1 events and 17706 original time points ...\n",
            "0 bad epochs dropped\n",
            "Not setting metadata\n",
            "1 matching events found\n",
            "No baseline correction applied\n",
            "0 projection items activated\n",
            "Using data from preloaded Raw for 1 events and 37502 original time points ...\n",
            "0 bad epochs dropped\n",
            "Not setting metadata\n",
            "1 matching events found\n",
            "No baseline correction applied\n",
            "0 projection items activated\n",
            "Using data from preloaded Raw for 1 events and 21835 original time points ...\n",
            "0 bad epochs dropped\n",
            "Not setting metadata\n",
            "1 matching events found\n",
            "No baseline correction applied\n",
            "0 projection items activated\n",
            "Using data from preloaded Raw for 1 events and 15955 original time points ...\n",
            "0 bad epochs dropped\n",
            "Not setting metadata\n",
            "1 matching events found\n",
            "No baseline correction applied\n",
            "0 projection items activated\n",
            "Using data from preloaded Raw for 1 events and 76065 original time points ...\n",
            "0 bad epochs dropped\n",
            "Not setting metadata\n",
            "1 matching events found\n",
            "No baseline correction applied\n",
            "0 projection items activated\n",
            "Using data from preloaded Raw for 1 events and 36962 original time points ...\n",
            "0 bad epochs dropped\n",
            "Not setting metadata\n",
            "1 matching events found\n",
            "No baseline correction applied\n",
            "0 projection items activated\n",
            "Using data from preloaded Raw for 1 events and 12849 original time points ...\n",
            "0 bad epochs dropped\n",
            "Not setting metadata\n",
            "1 matching events found\n",
            "No baseline correction applied\n",
            "0 projection items activated\n",
            "Using data from preloaded Raw for 1 events and 25834 original time points ...\n",
            "0 bad epochs dropped\n",
            "Not setting metadata\n",
            "1 matching events found\n",
            "No baseline correction applied\n",
            "0 projection items activated\n",
            "Using data from preloaded Raw for 1 events and 7194 original time points ...\n",
            "0 bad epochs dropped\n",
            "Not setting metadata\n",
            "1 matching events found\n",
            "No baseline correction applied\n",
            "0 projection items activated\n",
            "Using data from preloaded Raw for 1 events and 42484 original time points ...\n",
            "0 bad epochs dropped\n",
            "\n",
            "✅ Successfully created 14 epochs for Phase 1 and Phase 2.\n",
            "--- Calculating Power Spectral Density (PSD) features ---\n",
            "Effective window size : 1.000 (s)\n",
            "Effective window size : 1.000 (s)\n",
            "Effective window size : 1.000 (s)\n",
            "Effective window size : 1.000 (s)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s finished\n",
            "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s finished\n",
            "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s finished\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Effective window size : 1.000 (s)\n",
            "Effective window size : 1.000 (s)\n",
            "Effective window size : 1.000 (s)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s finished\n",
            "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s finished\n",
            "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s finished\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Effective window size : 1.000 (s)\n",
            "Effective window size : 1.000 (s)\n",
            "Effective window size : 1.000 (s)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s finished\n",
            "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s finished\n",
            "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s finished\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Effective window size : 1.000 (s)\n",
            "Effective window size : 1.000 (s)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s finished\n",
            "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s finished\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Effective window size : 1.000 (s)\n",
            "Effective window size : 1.000 (s)\n",
            "Effective window size : 1.000 (s)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.2s finished\n",
            "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s finished\n",
            "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s finished\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Effective window size : 1.000 (s)\n",
            "Effective window size : 1.000 (s)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s finished\n",
            "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s finished\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Effective window size : 1.000 (s)\n",
            "Effective window size : 1.000 (s)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s finished\n",
            "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s finished\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Effective window size : 1.000 (s)\n",
            "Effective window size : 1.000 (s)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s finished\n",
            "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s finished\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Effective window size : 1.000 (s)\n",
            "Effective window size : 1.000 (s)\n",
            "Effective window size : 1.000 (s)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s finished\n",
            "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s finished\n",
            "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s finished\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Effective window size : 1.000 (s)\n",
            "Effective window size : 1.000 (s)\n",
            "Effective window size : 1.000 (s)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s finished\n",
            "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s finished\n",
            "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s finished\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Effective window size : 1.000 (s)\n",
            "Feature matrix shape for Phase 1: (14, 10080)\n",
            "Feature matrix shape for Phase 2: (14, 10080)\n",
            "\n",
            "--- STEP 5: Training on Phase 1, Testing on Phase 2 ---\n",
            "\n",
            "================================================\n",
            "       TEMPORAL GENERALIZATION RESULTS\n",
            "================================================\n",
            "Model trained on 'Formation' Phase (Phase 1), tested on 'Persistent' Phase (Phase 2).\n",
            "Accuracy: 78.57%\n",
            "This result shows how well the 'formation' signature predicts the 'persistent' state.\n",
            "Chance level for Positive vs. Negative is ~50%.\n",
            "\n",
            "For reference, internal classification accuracy of Phase 2 data: 50.00%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s finished\n",
            "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s finished\n"
          ]
        }
      ]
    }
  ]
}